{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME HMM : décodage des lettres\n",
    "\n",
    "Le but est le même que dans le TME précédent: apprendre à classer les lettres les lettres manuscrites enregistrées par un stylo intelligent.\n",
    "\n",
    "Les étapes sont un peu les mêmes que dans le TME précédent... Avec un modèle plus complexe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('ressources/lettres.pkl', 'rb') as f:\n",
    "    data = pkl.load(f, encoding='latin1') \n",
    "X = np.array(data.get('letters')) # récupération des données sur les lettres\n",
    "Y = np.array(data.get('labels')) # récupération des étiquettes associées \n",
    "\n",
    "nCl = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage d'une lettre (cf TME précédent)\n",
    "def tracerLettre(let):\n",
    "    a = -let*np.pi/180; # conversion en rad\n",
    "    coord = np.array([[0, 0]]); # point initial\n",
    "    for i in range(len(a)):\n",
    "        x = np.array([[1, 0]]);\n",
    "        rot = np.array([[np.cos(a[i]), -np.sin(a[i])],[ np.sin(a[i]),np.cos(a[i])]])\n",
    "        xr = x.dot(rot) # application de la rotation\n",
    "        coord = np.vstack((coord,xr+coord[-1,:]))\n",
    "    plt.figure()\n",
    "    plt.plot(coord[:,0],coord[:,1])\n",
    "    #plt.savefig(\"exlettre.png\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrétisation (cf TME précédent)\n",
    "def discretise(x, d):\n",
    "    # votre code :\n",
    "    intervalle = 360 / d\n",
    "    xd = [np.floor(x[i] / intervalle) for i in range(len(x))]\n",
    "    return np.array(xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Apprentissage d'un modèle connaissant les états cachés du système\n",
    "\n",
    "### A1. Hypothèse gauche-droite\n",
    "\n",
    "On fait l'hypothèse que les états sont connus... Alors que ce n'est pas le cas. Mais il existe des stratégies simples (et parfois efficaces) pour attribuer arbitrairement des états sur les chaines.\n",
    "La plus classique est l'hypothèse gauche-droite, qui est bien adaptée aux signaux courts et non périodiques:\n",
    "\n",
    "* On définit le nombre d'états N\n",
    "* On découpe chaque série d'observations en N portions à peu près égales\n",
    "* On affecte l'état 0 au début, puis on incrémente jusqu'à l'état N pour la dernière portion de la chaine\n",
    "\n",
    "Sur un exemple:\n",
    "\n",
    "```\n",
    "X0 = [ 1.  9.  8.  8.  8.  8.  8.  9.  3.  4.  5.  6.  6.  6.  7.  7.  8.  9.  0.  0.  0.  1.  1.]\n",
    "S0 = [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.]\n",
    "```\n",
    "\n",
    "Au niveau de la mise en oeuvre, vous définirez la méthode ```def initGD(X,N):```qui prend un ensemble de séquences d'observations et qui retourne l'ensemble des séquences d'états. Pour chaque séquence $x$ vous pouvez utiliser:\n",
    "```\n",
    "np.floor(np.linspace(0,N-.00000001,len(x)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 1. 1. 2. 2. 2. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "def initGD(x,N):\n",
    "    return np.floor(np.linspace(0,N-.00000001,len(x)))\n",
    "    \n",
    "# 14 observations aléatoires\n",
    "x = np.floor(np.random.rand(14)*10)\n",
    "print(initGD(x,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.214493 347.719116 322.088898 312.230957 314.851013 315.487213\n",
      " 313.556702 326.534973 141.288971 167.606689 199.321594 217.911087\n",
      " 226.443298 235.002472 252.354492 270.045654 291.665161 350.934723\n",
      "  17.892815  20.281025  28.207161  43.883423  53.459026] [0. 0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Construire la base Q des états, initialisés en Gauche-Droite correspondant aux observations X\n",
    "N = 5 # nombre d'états\n",
    "\n",
    "Q = # A compléter\n",
    "print(X[0],\"\\n\",Q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "```\n",
    "[ 36.214493 347.719116 322.088898 312.230957 314.851013 315.487213\n",
    " 313.556702 326.534973 141.288971 167.606689 199.321594 217.911087\n",
    " 226.443298 235.002472 252.354492 270.045654 291.665161 350.934723\n",
    "  17.892815  20.281025  28.207161  43.883423  53.459026] \n",
    "[0. 0. 0. 0. 0. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Apprentissage\n",
    "\n",
    "Etant donné la structure d'un MMC:\n",
    "* les observations n'influencent pas les états: les matrices $\\Pi, A$ s'obtiennent comme dans un modèle de Markov simple \n",
    "* chaque observation ne dépend que de l'état courant\n",
    "\n",
    "La nature des données nous pousse à considérer des lois de probabilités discrètes quelconques pour les émissions. L'idée est donc de procéder par comptage en définissant la matrice $B$ comme suit:\n",
    "* $K$ colonnes (nombre d'observations), $N$ lignes (nombre d'états)\n",
    "* Chaque ligne correspond à une loi d'émission pour un état (ie, chaque ligne somme à 1)\n",
    "Ce qui donne l'algorithme:\n",
    "1. $b_{ij}$ = comptage des émissions depuis l'état $s_i$ vers l'observation $x_j$\n",
    "1. normalisation des lignes de $B$\n",
    "\n",
    "Donner le code de la fonction ```def learnHMM(allX, allS, N, K):``` qui apprend un modèle à partir d'un ensemble de couples (seq. d'observations, seq. d'états)\n",
    "\n",
    "***Variante stabilisée***: afin d'éviter les transitions à probabilité nulle et de régulariser l'ensemble du système, il est intéressant d'initialiser les matrices de transition à une valeur non nulle. Cette initialisation *spéciale* peut être faite de manière optionnelle en utilisant les arguments par défaut de python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnHMM(allx, allq, N, K, initTo0=False, eps = 1e-5):\n",
    "    if initTo0:\n",
    "        A = np.zeros((N,N))\n",
    "        B = np.zeros((N,K))\n",
    "        Pi = np.zeros(N)\n",
    "    else:\n",
    "        A = np.zeros((N,N)) # np.ones((N,N))*eps => non pertinent pour un modèle GD\n",
    "        B = np.ones((N,K))*eps\n",
    "        Pi = np.ones(N)*eps\n",
    "    # suite de votre code\n",
    "    pass\n",
    "\n",
    "    return Pi , A, B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0.] \n",
      " [[0.79 0.21 0.   0.   0.  ]\n",
      " [0.   0.76 0.24 0.   0.  ]\n",
      " [0.   0.   0.77 0.23 0.  ]\n",
      " [0.   0.   0.   0.76 0.24]\n",
      " [0.   0.   0.   0.   1.  ]] \n",
      " [[0.06 0.02 0.   0.   0.   0.   0.   0.04 0.49 0.4 ]\n",
      " [0.   0.04 0.   0.13 0.09 0.13 0.02 0.09 0.41 0.09]\n",
      " [0.   0.   0.   0.02 0.12 0.5  0.31 0.04 0.   0.  ]\n",
      " [0.07 0.   0.   0.   0.   0.   0.26 0.33 0.2  0.15]\n",
      " [0.73 0.12 0.   0.   0.   0.   0.   0.02 0.02 0.12]]\n"
     ]
    }
   ],
   "source": [
    "# le nombre d'états a été fixé au dessus (à 5)\n",
    "K = 10 # discrétisation (=10 observations possibles)\n",
    "Xd = discretise(X,K)\n",
    "Pi, A, B = learnHMM(Xd[Y=='a'],Q[Y=='a'],N,K, True)\n",
    "\n",
    "print(Pi,\"\\n\", np.around(A,decimals=2), \"\\n\", np.around(B,decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation sur les séquences de la classe 'a':\n",
    "```\n",
    "  Pi=[ 1.  0.  0.  0.  0.]\n",
    "\n",
    "  A=[[0.79 0.21 0.   0.   0.  ]\n",
    "     [0.   0.76 0.24 0.   0.  ]\n",
    "     [0.   0.   0.77 0.23 0.  ]\n",
    "     [0.   0.   0.   0.76 0.24]\n",
    "     [0.   0.   0.   0.   1.  ]] \n",
    "\n",
    "  B=[[0.06 0.02 0.   0.   0.   0.   0.   0.04 0.49 0.4 ]\n",
    "     [0.   0.04 0.   0.13 0.09 0.13 0.02 0.09 0.41 0.09]\n",
    "     [0.   0.   0.   0.02 0.12 0.5  0.31 0.04 0.   0.  ]\n",
    "     [0.07 0.   0.   0.   0.   0.   0.26 0.33 0.2  0.15]\n",
    "     [0.73 0.12 0.   0.   0.   0.   0.   0.02 0.02 0.12]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Algorithme de décodage, Viterbi\n",
    "\n",
    "Retrouver **la sequence d'état caché la plus probable** ayant permis de générer la séquence d'observation.\n",
    "\n",
    "Rappels sur l'algorithme Viterbi (1967):\n",
    "\n",
    "- Il sert à estimer la séquence d'états la plus probable étant donnés les observations et le modèle.\n",
    "- Il peut servir à approximer la probabilité de la séquence d'observation étant donné le modèle. \n",
    "\n",
    "1\\. Initialisation (avec les indices à 0 en python): \n",
    "\n",
    "$$\\begin{array}{ccccccccc} \\delta_{0} (i) &=& \\log \\pi_{i} +\\log b_{i} (x_{1}) \\\\ \\Psi_{0}(i) &=& -1 \\mbox{ Note: -1 car non utilisé normalement} \\end{array}$$\n",
    "\n",
    "2\\. Récursion: \n",
    "\n",
    "$$ \\begin{array}{ccccccccc} \\delta_{t} (j) &=&\\displaystyle \\left[\\max_{i} \\delta_{t-1}(i) + \\log a_{ij}\\right] + \\log b_{j}(x_{t}) \\\\ \\Psi_{t}(j) &=&\\displaystyle \\arg\\max_{i\\in [1,\\ N]} \\delta_{t-1} (i) + \\log a_{ij} \\end{array}$$\n",
    "\n",
    "3\\. Terminaison (indices à {$T-1$} en python) \n",
    "\n",
    "$$ S^{\\star} = \\max_{i} \\delta_{T-1}(i)$$\n",
    "\n",
    "4\\. Chemin $$\\begin{array}{ccccccccc} s_{T-1}^{\\star} & = &\\displaystyle \\arg\\max_{i} \\delta_{T-1}(i) \\\\ s_{t}^{\\star} & = & \\displaystyle \\Psi_{t+1}(s_{t+1}^{\\star}) \\end{array}$$\n",
    "\n",
    "L'estimation de $\\log p(x_0^{T-1} | \\lambda)$ est obtenue en cherchant la plus grande probabilité dans la dernière colonne de $\\delta$. Donner le code de la méthode `viterbi(x,Pi,A,B):` \n",
    "\n",
    "### B.1 Définition de la fonction viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x,Pi,A,B):\n",
    "    T = len(x)\n",
    "    N = len(Pi)\n",
    "    logPI = np.log(Pi)\n",
    "    logA = np.log(A)\n",
    "    logB = np.log(B)\n",
    "    logdelta = np.zeros((N,T))\n",
    "    psi = np.zeros((N,T))\n",
    "    S = np.zeros(T) # les états à retourner\n",
    "    # votre code\n",
    "    # pass\n",
    "\n",
    "    return S, logp # états + approximation de la vraimsemblance des obs par rapport au modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 9. 8. 8. 8. 8. 8. 9. 3. 4. 5. 6. 6. 6. 7. 7. 8. 9. 0. 0. 0. 1. 1.] \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.] -38.09356554559258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "q, p = viterbi(Xd[0], Pi, A, B) # attention à bien donner la version discrétisée des observations\n",
    "\n",
    "print(Xd[0],\"\\n\",q, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "```\n",
    "# décodage\n",
    "[1. 9. 8. 8. 8. 8. 8. 9. 3. 4. 5. 6. 6. 6. 7. 7. 8. 9. 0. 0. 0. 1. 1.] \n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 4. 4. 4. 4. 4.] \n",
    "# log vraisemblance\n",
    "-38.09356554559258\n",
    "```\n",
    "\n",
    "### B2. Utilisation de l'algorithme de Viterbi pour la classification\n",
    "\n",
    "Nous aurions normalement du utiliser l'algorithme forward pour estimer la vraisemblance... Mais l'algorithme de Viterbi est plus versatile et il nous permettra de mettre en oeuvre des stratégies d'apprentissage dans les questions suivantes.\n",
    "\n",
    "Nous allons donc utiliser la vraisemblance du chemin le plus probable (dans l'espace des états) comme une approximation de la vraisemblance de l'ensemble des chemins.\n",
    "\n",
    "1. Calculer un modèle par lettre\n",
    "1. Estimer la vraisemblance des lettres dans chaque modèle\n",
    "1. Donner un score de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# calcul des 26 modèles\n",
    "models = \n",
    "\n",
    "# affectation des signaux = calcul de vraisemblance[n_sig x n_modèles]\n",
    "all_proba = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de bonne classification 0.8507462686567164\n"
     ]
    }
   ],
   "source": [
    "ypred = np.array(all_proba).argmax(1) # à modifier si votre all_proba est transposé\n",
    "\n",
    "traduction = {ch:i for i,ch in enumerate(np.unique(Y))}\n",
    "Ynum = np.array([traduction[y] for y in Y])\n",
    "\n",
    "tx_bonne_classif = np.where(ypred == Ynum,1.,0).mean()\n",
    "print(\"Taux de bonne classification\" , tx_bonne_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : 85.07% de bonne classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Séparation apprentissage / test\n",
    "\n",
    "L'expérience précédente n'est pas très satisfaisante, l'évalution des performances n'étant pas fiable.\n",
    "Nous allons donc séparer les données en un jeu d'apprentissage et un jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOUT LE CODE EST FOURNI ICI\n",
    "\n",
    "# séparation stratifiée par classe (non randomisée)\n",
    "def separation_app_test(Y, pc_app):\n",
    "    ind_app  = []\n",
    "    ind_test = []\n",
    "    for cl in np.unique(Y):\n",
    "        index = np.where(Y == cl)[0]\n",
    "        ind_app += index[:int(len(index)*pc_app)].tolist()\n",
    "        ind_test += index[int(len(index)*pc_app):].tolist()\n",
    "    return ind_app, ind_test\n",
    "\n",
    "pc_app = 0.8 # pourcentage de points en apprentissage\n",
    "ind_app, ind_test = separation_app_test(Y, pc_app)\n",
    "# separation des données\n",
    "Xd_app, Xd_test = Xd[ind_app], Xd[ind_test]\n",
    "Y_app, Y_test   = Y[ind_app], Y[ind_test]\n",
    "Yn_app, Yn_test = Ynum[ind_app], Ynum[ind_test]\n",
    "Q_app, Q_test   = Q[ind_app], Q[ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de bonne classification 0.8509615384615384 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "# apprentissage des modèles sur les données d'apprentissage seulement\n",
    "\n",
    "models = \n",
    "\n",
    "# calcul des affectations\n",
    "all_proba_app = \n",
    "all_proba_test = \n",
    "\n",
    "# calcul des performances\n",
    "ypred_app  = np.array(all_proba_app).argmax(1) # a verifier en fonction de vos amtrice all_proba\n",
    "ypred_test = np.array(all_proba_test).argmax(1)\n",
    "tx_bonne_classif_app = np.where(ypred_app == Yn_app,1.,0).mean()\n",
    "tx_bonne_classif_test = np.where(ypred_test == Yn_test,1.,0).mean()\n",
    "print(\"Taux de bonne classification\" , tx_bonne_classif_app, tx_bonne_classif_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check :\n",
    "\n",
    "Taux de bonne classification 0.8509615384615384 \n",
    "0.7666666666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4. Matrice de confusion\n",
    "\n",
    "Afin de mieux comprendre les erreurs, tracer la matrice de confusion associée aux lettre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_conf(Y, Yhat):\n",
    "    nCl = len(np.unique(Y)) # on espere que le vecteur des Y contient toutes les valeurs entre 0 et nCl-1\n",
    "    conf = np.zeros((nCl,nCl))\n",
    "    for y,yh in zip(Y,Yhat):\n",
    "        conf[y, yh] += 1\n",
    "    return conf\n",
    "\n",
    "# appel à la fonction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B5. OPT. Trouver les paramètres de discrétisation et de nombre d'états optimaux vis à vis des données de test\n",
    "\n",
    "Cette procédrure s'appelle un grid-search: proposer des valeurs puis tester de manière exhaustive. \n",
    "Cette procédure est plutot simple à implémenter... mais longue en temps de calcul.\n",
    "\n",
    "**NE PAS FAIRE EN TP**\n",
    "\n",
    "**Note:** à force de faire des évaluations sur les données de test, on introduit un risque de sur-apprentissage de ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Baum-Welch simplifié\n",
    "\n",
    "En utilisant la procédure de Baum-Welch simplifiée vue en TD et rappelée ci-dessous, proposer un code pour apprendre (ou plutot raffiner) les modèles correspondant aux 26 lettres de l'alphabet.\n",
    "\n",
    "**Baum-Welch simplifié:**\n",
    "1. Initialisation des états cachés arbitraire (eg méthode gauche-droite)\n",
    "1. Tant que ''critère de convergence'' non atteint\n",
    "    1. Apprentissage des modèles $\\lambda_{lettre}=\\{\\Pi, A, B\\}$\n",
    "    1. Estimation des états cachés par Viterbi\n",
    "\n",
    "Le critère de convergence sera la vraisemblance.\n",
    "* A chaque itération $k$ et pour toutes les lettres $lettre$, calculer pour l'ensemble des séquences d'observation : \n",
    "$$\\log\\mathcal L^k = \\sum_{lettre}\\sum_i \\log p(\\mathbf x_i^{lettre} | \\lambda_{lettre}^k)$$\n",
    "* Lorsque la vraisemblance n'évolue plus (ie $\\frac{\\log\\mathcal L^k - \\log\\mathcal L^{k+1}}{\\log\\mathcal L^k} < 1e-4$), sortir de la boucle de mise à jour.\n",
    "\n",
    "\n",
    "* Donner l'implémentation de la méthode d'apprentissage\n",
    "* Tracer la courbe de l'évolution de la vraisemblance au cours des itérations\n",
    "\n",
    "### C1. Donner le code de la fonction baumwelch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10\n",
      "0 -4448.105413831695\n",
      "1 -3723.2800152952045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -3718.5731479934116\n",
      "3 -3713.054181192417\n",
      "4 -3711.8252459541063\n",
      "5 -3711.8252459541063\n",
      "cvg in 5 iterations\n"
     ]
    }
   ],
   "source": [
    "# nIter : nombre d'iteration max (pour éviter les boucles infinies)\n",
    "# Xd : les données d'observations discrétisées\n",
    "# Yn : les étiquettes au format numérique\n",
    "# models: une initialisation maline des modèles à apprendre\n",
    "#     - RAPPEL: l'initialisation est critique !\n",
    "\n",
    "def baumwelch(nIter, Xd, Yn, models):\n",
    "    N = len(models[0][0])\n",
    "    K = models[0][2].shape[1] # la seconde dimension de B\n",
    "    print(N,K)\n",
    "    # Baum Welch (simplifie)\n",
    "    nCl = len(np.unique(Y))\n",
    "    nIter = 10\n",
    "    # A compléter\n",
    "    \n",
    "    return models\n",
    "\n",
    "models = baumwelch(30, Xd_app, Yn_app, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C2. Evaluation des performances\n",
    "\n",
    "Les performances sont-elles meilleures après optimisation du modèle?\n",
    "\n",
    "Faudrait-il refaire une procédure de grid-search basée sur l'algorithme Baum-Welch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n",
      "/Users/vguigue/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de bonne classification 0.8653846153846154 0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "# calcul des affectations en apprentissage et en test\n",
    "all_proba_app = \n",
    "all_proba_test = \n",
    "\n",
    "# calcul des perfromances\n",
    "ypred_app  = np.array(all_proba_app).argmax(1)\n",
    "ypred_test = np.array(all_proba_test).argmax(1)\n",
    "tx_bonne_classif_app = np.where(ypred_app == Yn_app,1.,0).mean()\n",
    "tx_bonne_classif_test = np.where(ypred_test == Yn_test,1.,0).mean()\n",
    "print(\"Taux de bonne classification\" , tx_bonne_classif_app, tx_bonne_classif_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Génération de lettres\n",
    "\n",
    "De la même qu'un modèle de Markov, la chaine de Markov cachée est un modèle génératif: il est possible de générer des lettre.\n",
    "\n",
    "Avec la version discrétisée des angles, ça ne va toujours pas donner de résultats formidables... Mais c'est possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
